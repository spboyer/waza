# Integration Testing with Copilot SDK

This guide explains how to run real integration tests using the GitHub Copilot SDK.

## Prerequisites

1. **Install the Copilot SDK dependency:**
   ```bash
   pip install skill-eval[copilot]
   ```

2. **Authenticate with Copilot CLI:**
   ```bash
   copilot
   # Follow prompts to authenticate
   ```

## Configuration

### Eval Spec Configuration

Update your `eval.yaml` to use the Copilot SDK executor:

```yaml
name: my-skill-eval
skill: my-skill
version: "1.0"

config:
  trials_per_task: 3
  executor: copilot-sdk           # Use real Copilot SDK
  model: claude-sonnet-4-20250514  # Specify model
  timeout_seconds: 300
  
  # Skill directories for the SDK to load
  skill_directories:
    - ./skills
    - /path/to/other/skills
  
  # MCP server configurations (optional)
  mcp_servers:
    azure:
      type: stdio
      command: npx
      args: ["-y", "@azure/mcp", "server", "start"]
```

### CLI Override

You can override the executor and model at runtime:

```bash
# Run with Copilot SDK instead of mock
skill-eval run eval.yaml --executor copilot-sdk --model claude-sonnet-4-20250514

# Run with verbose output to see conversation in real-time
skill-eval run eval.yaml --executor copilot-sdk -v

# Provide project context files
skill-eval run eval.yaml --executor copilot-sdk --context-dir ./my-project

# Save conversation transcript for debugging
skill-eval run eval.yaml --executor copilot-sdk --log transcript.json

# Full debugging session
skill-eval run eval.yaml \
  --executor copilot-sdk \
  --model claude-sonnet-4-20250514 \
  --context-dir ./fixtures \
  --log transcript.json \
  --output results.json \
  -v

# Compare different models
skill-eval run eval.yaml --model gpt-4o -o results-gpt4o.json
skill-eval run eval.yaml --model claude-sonnet-4-20250514 -o results-claude.json
skill-eval compare results-gpt4o.json results-claude.json
```

## Executor Types

| Executor | Description | Use Case |
|----------|-------------|----------|
| `mock` | Simulates responses | Unit tests, CI without API keys |
| `copilot-sdk` | Real Copilot agent sessions | Integration tests, benchmarking |

## CopilotExecutor Features

The `CopilotExecutor` wraps the `@github/copilot-sdk` to provide:

- **Real LLM responses** from specified models
- **Skill invocation tracking** - verify your skill was called
- **Tool call validation** - ensure expected tools were used
- **Session event capture** - full transcript for analysis
- **Workspace isolation** - each trial runs in a temp directory

### Execution Result

Each execution returns an `ExecutionResult` with:

```python
result = await executor.execute(
    prompt="Deploy my app to Azure",
    context={"files": [{"path": "app.py", "content": "..."}]},
    skill_name="azure-deploy"
)

# Access results
print(result.output)              # Final assistant response
print(result.events)              # Session events (transcript)
print(result.tool_calls)          # Tools that were called
print(result.is_skill_invoked("azure-deploy"))  # Check skill activation
print(result.contains_keyword("deployed"))       # Check for keywords
```

## Model Comparison

Compare results across different models:

```bash
# Run the same eval with different models
skill-eval run eval.yaml --model gpt-4o -o results/gpt-4o.json
skill-eval run eval.yaml --model claude-sonnet-4-20250514 -o results/claude.json
skill-eval run eval.yaml --model gpt-4o-mini -o results/gpt-4o-mini.json

# Generate comparison report
skill-eval compare results/*.json -o comparison-report.md
```

### Comparison Output

```
Model Comparison Report

              Summary Comparison              
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Metric          â”ƒ gpt-4o â”ƒ claude  â”ƒ gpt-4o-mini â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Pass Rate       â”‚ 100.0% â”‚  95.0%  â”‚      85.0%  â”‚
â”‚ Composite Score â”‚   0.98 â”‚   0.92  â”‚        0.81 â”‚
â”‚ Tasks Passed    â”‚   20/20â”‚  19/20  â”‚       17/20 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ† Best: gpt-4o (score: 0.98)
```

## CI/CD Integration

### Skip Integration Tests in CI

Integration tests require authentication and are typically skipped in CI:

```yaml
# .github/workflows/test.yaml
- name: Run unit tests
  run: skill-eval run eval.yaml --executor mock
  
- name: Run integration tests (manual only)
  if: github.event_name == 'workflow_dispatch'
  run: skill-eval run eval.yaml --executor copilot-sdk
```

### Environment Variables

| Variable | Effect |
|----------|--------|
| `CI=true` | Auto-detected in CI; forces mock executor |
| `SKIP_INTEGRATION_TESTS=true` | Explicitly skip real SDK tests |

## Troubleshooting

### "Copilot SDK not installed"

```bash
pip install skill-eval[copilot]
# or
pip install copilot-sdk
```

### "Authentication failed"

Run the Copilot CLI and authenticate:
```bash
copilot
```

### "Session timed out"

Increase timeout in config:
```yaml
config:
  timeout_seconds: 600  # 10 minutes
```

## Example: Full Integration Test

```yaml
# eval.yaml
name: azure-deploy-integration
skill: azure-deploy
version: "1.0"

config:
  trials_per_task: 3
  executor: copilot-sdk
  model: claude-sonnet-4-20250514
  timeout_seconds: 300
  skill_directories:
    - ../../skills

tasks:
  - tasks/*.yaml

graders:
  - type: code
    name: skill_invoked
    config:
      assertions:
        - "'azure-deploy' in str(transcript)"
  
  - type: regex
    name: deployment_link
    config:
      must_match:
        - "azurewebsites\\.net|azurestaticapps\\.net"
```

```bash
# Run with real Copilot SDK
skill-eval run eval.yaml --executor copilot-sdk -o integration-results.json
```
