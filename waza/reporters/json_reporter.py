"""JSON reporter for eval results."""

from __future__ import annotations

import json
from pathlib import Path

from waza.schemas.results import EvalResult


class JSONReporter:
    """Outputs eval results as JSON."""

    def __init__(self, indent: int = 2, include_transcripts: bool = False):
        """Initialize reporter.

        Args:
            indent: JSON indentation level
            include_transcripts: Whether to include full transcripts in output
        """
        self.indent = indent
        self.include_transcripts = include_transcripts

    def report(self, result: EvalResult) -> str:
        """Generate JSON report string."""
        data = result.model_dump(mode="json")

        # Optionally strip transcripts for smaller output
        if not self.include_transcripts:
            for task in data.get("tasks", []):
                for trial in task.get("trials", []):
                    trial.pop("transcript", None)

        return json.dumps(data, indent=self.indent, default=str)

    def report_to_file(self, result: EvalResult, path: str | Path) -> None:
        """Write JSON report to file."""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(self.report(result))

    def report_summary(self, result: EvalResult) -> str:
        """Generate summary-only JSON report."""
        summary_data = {
            "eval_id": result.eval_id,
            "skill": result.skill,
            "eval_name": result.eval_name,
            "timestamp": result.timestamp.isoformat(),
            "summary": result.summary.model_dump(),
            "metrics": {k: v.model_dump() for k, v in result.metrics.items()},
        }
        return json.dumps(summary_data, indent=self.indent)


class MarkdownReporter:
    """Outputs eval results as Markdown."""

    def report(self, result: EvalResult) -> str:
        """Generate Markdown report string."""
        lines = [
            f"# Eval Results: {result.eval_name}",
            "",
            f"**Skill:** {result.skill}  ",
            f"**Eval ID:** `{result.eval_id}`  ",
            f"**Timestamp:** {result.timestamp.isoformat()}  ",
            "",
            "## Summary",
            "",
            "| Metric | Value |",
            "|--------|-------|",
            f"| Total Tasks | {result.summary.total_tasks} |",
            f"| Passed | {result.summary.passed} |",
            f"| Failed | {result.summary.failed} |",
            f"| Pass Rate | {result.summary.pass_rate:.1%} |",
            f"| Composite Score | {result.summary.composite_score:.2f} |",
            f"| Duration | {result.summary.duration_ms}ms |",
            "",
        ]

        # Metrics table
        if result.metrics:
            lines.extend([
                "## Metrics",
                "",
                "| Metric | Score | Threshold | Status |",
                "|--------|-------|-----------|--------|",
            ])
            for name, metric in result.metrics.items():
                status = "âœ…" if metric.passed else "âŒ"
                lines.append(f"| {name} | {metric.score:.2f} | {metric.threshold:.2f} | {status} |")
            lines.append("")

        # Task results
        lines.extend([
            "## Task Results",
            "",
            "| Task | Status | Pass Rate | Mean Score |",
            "|------|--------|-----------|------------|",
        ])
        for task in result.tasks:
            status_icon = {"passed": "âœ…", "failed": "âŒ", "partial": "âš ï¸", "error": "ğŸ’¥"}.get(task.status, "â“")
            pass_rate = task.aggregate.pass_rate if task.aggregate else 0
            mean_score = task.aggregate.mean_score if task.aggregate else 0
            lines.append(f"| {task.name} | {status_icon} | {pass_rate:.1%} | {mean_score:.2f} |")

        lines.append("")
        lines.append("---")
        lines.append("*Generated by waza*")

        return "\n".join(lines)

    def report_to_file(self, result: EvalResult, path: str | Path) -> None:
        """Write Markdown report to file."""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(self.report(result))


class GitHubReporter:
    """Outputs eval results for GitHub Actions."""

    def report_summary(self, result: EvalResult) -> str:
        """Generate GitHub Actions job summary."""
        status_emoji = "âœ…" if result.summary.pass_rate >= 0.8 else "âŒ"

        lines = [
            f"## {status_emoji} Skill Eval: {result.skill}",
            "",
            f"**Pass Rate:** {result.summary.pass_rate:.1%} ({result.summary.passed}/{result.summary.total_tasks} tasks)",
            f"**Composite Score:** {result.summary.composite_score:.2f}",
            "",
        ]

        # Metrics
        if result.metrics:
            lines.append("### Metrics")
            lines.append("")
            for name, metric in result.metrics.items():
                icon = "âœ…" if metric.passed else "âŒ"
                lines.append(f"- {icon} **{name}**: {metric.score:.2f} (threshold: {metric.threshold:.2f})")
            lines.append("")

        # Failed tasks
        failed_tasks = [t for t in result.tasks if t.status != "passed"]
        if failed_tasks:
            lines.append("### Failed Tasks")
            lines.append("")
            for task in failed_tasks[:5]:  # Limit to 5
                lines.append(f"- âŒ {task.name}")
            if len(failed_tasks) > 5:
                lines.append(f"- ... and {len(failed_tasks) - 5} more")

        return "\n".join(lines)

    def set_outputs(self, result: EvalResult) -> dict[str, str]:
        """Generate GitHub Actions output variables."""
        return {
            "pass_rate": str(result.summary.pass_rate),
            "composite_score": str(result.summary.composite_score),
            "passed": str(result.summary.passed),
            "failed": str(result.summary.failed),
            "total": str(result.summary.total_tasks),
            "status": "success" if result.summary.pass_rate >= 0.8 else "failure",
        }
